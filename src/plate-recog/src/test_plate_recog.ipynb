{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb5a7bd-4672-4b3d-8fec-967d69b65cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcfd840-2d51-4691-bc8e-3dd707fd6c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/10/11 01:18:24] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/duc-softzone/.paddleocr/whl/det/ch/ch_PP-OCRv4_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/duc-softzone/intelligent-surveillance-system/src/plate-recog/inference/rec_r45_abinet_train', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/home/duc-softzone/anaconda3/envs/rtfa/lib/python3.10/site-packages/paddleocr/ppocr/utils/ppocr_keys_v1.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/home/duc-softzone/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='ch', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "model = PaddleOCR(rec=True, rec_model_dir='/home/duc-softzone/intelligent-surveillance-system/src/plate-recog/inference/rec_r45_abinet_train', use_angle_cls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c31148-09d9-4fd0-9d4f-f322a2f058eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def rec(path):\n",
    "    start_time = time()\n",
    "    result = model.ocr(path, rec=True)\n",
    "    try:\n",
    "        text = result[0][0][0]  # Extract the text\n",
    "        confidence = result[0][0][1]  # Extract confidence score\n",
    "        \n",
    "        end_time = time()\n",
    "        print(path, text, confidence, 'Duration:', (end_time - start_time) * 1000, 'ms')\n",
    "    except Exception as e:\n",
    "        print(path, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1192c797-154e-434f-9085-7b12402310a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BASE_PATH = 'data/train/images'\n",
    "dir_list = os.listdir(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d5f3820-e391-4edb-8e0d-ad66886db097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/10/11 01:19:20] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.11468100547790527\n",
      "[2024/10/11 01:19:20] ppocr DEBUG: cls num  : 3, elapsed : 0.01846790313720703\n",
      "[2024/10/11 01:19:20] ppocr DEBUG: rec_res num  : 3, elapsed : 0.1681053638458252\n",
      "data/train/images/001074_jpg.rf.6f6406b7c8f5dd3ede891476267564eb.jpg [[125.0, 190.0], [289.0, 182.0], [292.0, 232.0], [128.0, 240.0]] ('NBD-0198', 0.9314591884613037) Duration: 312.0553493499756 ms\n",
      "[2024/10/11 01:19:20] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.10318589210510254\n",
      "[2024/10/11 01:19:20] ppocr DEBUG: cls num  : 3, elapsed : 0.014670133590698242\n",
      "[2024/10/11 01:19:20] ppocr DEBUG: rec_res num  : 3, elapsed : 0.14493989944458008\n",
      "data/train/images/001725_jpg.rf.9b6b05a541c7a6e6f1490d61e2b51a75.jpg [[174.0, 198.0], [239.0, 196.0], [240.0, 221.0], [175.0, 223.0]] ('MRY-0890', 0.9819923639297485) Duration: 270.9674835205078 ms\n",
      "[2024/10/11 01:19:20] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.10193610191345215\n",
      "[2024/10/11 01:19:20] ppocr DEBUG: cls num  : 3, elapsed : 0.014339447021484375\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: rec_res num  : 3, elapsed : 0.1448376178741455\n",
      "data/train/images/000883_jpg.rf.e84cb0ec653ba48ac6408013725d42c0.jpg [[136.0, 193.0], [283.0, 195.0], [282.0, 240.0], [135.0, 238.0]] ('301·DTH', 0.9104126691818237) Duration: 269.74987983703613 ms\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.10087013244628906\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: cls num  : 2, elapsed : 0.013298273086547852\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: rec_res num  : 2, elapsed : 0.10274481773376465\n",
      "data/train/images/000472_jpg.rf.ff91707fe85e7cc4ba68251391042ec0.jpg [[117.0, 190.0], [295.0, 182.0], [297.0, 234.0], [119.0, 242.0]] ('ENU-8221', 0.9860600233078003) Duration: 224.67517852783203 ms\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.1021108627319336\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: cls num  : 4, elapsed : 0.02054762840270996\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: rec_res num  : 4, elapsed : 0.1993265151977539\n",
      "data/train/images/001604_jpg.rf.f859d1efd3834937199a6aa9b0687cb7.jpg [[160.0, 158.0], [315.0, 158.0], [315.0, 185.0], [160.0, 185.0]] ('台河省#', 0.6399646401405334) Duration: 331.85744285583496 ms\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.1003103256225586\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: cls num  : 2, elapsed : 0.01306772232055664\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: rec_res num  : 2, elapsed : 0.10618400573730469\n",
      "data/train/images/001286_jpg.rf.c3fc67b0c9cdaa6a04b9e7512bc7e58f.jpg [[104.0, 185.0], [317.0, 188.0], [316.0, 244.0], [103.0, 241.0]] ('MKM-7967', 0.9290755987167358) Duration: 227.7531623840332 ms\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.1046140193939209\n",
      "[2024/10/11 01:19:21] ppocr DEBUG: cls num  : 1, elapsed : 0.01218867301940918\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: rec_res num  : 1, elapsed : 0.06444740295410156\n",
      "data/train/images/000699_jpg.rf.af4122f2ffc9ec8d9d01c61566a951b7.jpg [[139.0, 189.0], [279.0, 187.0], [280.0, 230.0], [140.0, 232.0]] ('MPC·1078', 0.9420017004013062) Duration: 190.4609203338623 ms\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.10702180862426758\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: cls num  : 2, elapsed : 0.012578010559082031\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: rec_res num  : 2, elapsed : 0.10707354545593262\n",
      "data/train/images/002875_jpg.rf.e56e9f1378124cc8c413a3efc59922dc.jpg [[0.0, 146.0], [81.0, 136.0], [87.0, 185.0], [1.0, 195.0]] ('R·540', 0.8879605531692505) Duration: 235.40711402893066 ms\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.10743546485900879\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: cls num  : 2, elapsed : 0.012801408767700195\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: rec_res num  : 2, elapsed : 0.10442018508911133\n",
      "data/train/images/001253_jpg.rf.d55b476c01d703291f23dcaec06155c3.jpg [[129.0, 188.0], [300.0, 200.0], [296.0, 250.0], [125.0, 238.0]] ('771-HDF', 0.8742144703865051) Duration: 234.9386215209961 ms\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.10051107406616211\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: cls num  : 1, elapsed : 0.01138925552368164\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: rec_res num  : 1, elapsed : 0.060884952545166016\n",
      "data/train/images/002788_jpg.rf.7501d546400def703ffdaa16bf09f4fb.jpg [[40.0, 217.0], [285.0, 95.0], [343.0, 212.0], [98.0, 334.0]] ('NFD-1799', 0.869809627532959) Duration: 184.68332290649414 ms\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.10084891319274902\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: cls num  : 1, elapsed : 0.011672019958496094\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: rec_res num  : 1, elapsed : 0.060544729232788086\n",
      "data/train/images/000925_jpg.rf.c9fbb218dcb717cd15279007b5e22a32.jpg [[122.0, 193.0], [284.0, 188.0], [285.0, 234.0], [123.0, 239.0]] ('NFX-5361', 0.9376020431518555) Duration: 183.6838722229004 ms\n",
      "[2024/10/11 01:19:22] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.10132503509521484\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: cls num  : 2, elapsed : 0.012712478637695312\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: rec_res num  : 2, elapsed : 0.10318827629089355\n",
      "data/train/images/002924_jpg.rf.d3a156a0395fb699ae862c2317e57cfd.jpg [[0.0, 123.0], [41.0, 123.0], [41.0, 157.0], [0.0, 157.0]] ('CBF', 0.9952001571655273) Duration: 225.5702018737793 ms\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: dt_boxes num : 0, elapsed : 0.09793400764465332\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: cls num  : 0, elapsed : 0\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: rec_res num  : 0, elapsed : 2.6226043701171875e-06\n",
      "data/train/images/002221_jpg.rf.e5eacdb318cd52f2e28a866e5dce2f1b.jpg 'NoneType' object is not subscriptable\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.10088706016540527\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: cls num  : 2, elapsed : 0.012787342071533203\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: rec_res num  : 2, elapsed : 0.1060183048248291\n",
      "data/train/images/000219_jpg.rf.4dd8a82ad8381962a3ad320ad8839484.jpg [[167.0, 14.0], [217.0, 11.0], [218.0, 25.0], [168.0, 28.0]] ('YAMAHA', 0.9568489193916321) Duration: 267.5209045410156 ms\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.10072731971740723\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: cls num  : 2, elapsed : 0.012641668319702148\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: rec_res num  : 2, elapsed : 0.11581873893737793\n",
      "data/train/images/002930_jpg.rf.cc39e17d9e1a11803c40c35029c3336f.jpg [[316.0, 126.0], [366.0, 111.0], [374.0, 137.0], [324.0, 152.0]] ('MUS-2985', 0.9710151553153992) Duration: 299.22962188720703 ms\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.11821675300598145\n",
      "[2024/10/11 01:19:23] ppocr DEBUG: cls num  : 4, elapsed : 0.019097328186035156\n",
      "[2024/10/11 01:19:24] ppocr DEBUG: rec_res num  : 4, elapsed : 0.21817612648010254\n",
      "data/train/images/000613_jpg.rf.6f4e10f07855265c25af78fc84f6eb6f.jpg [[105.0, 174.0], [311.0, 190.0], [307.0, 243.0], [101.0, 227.0]] ('NDA-1318', 0.9519110918045044) Duration: 376.2927055358887 ms\n",
      "[2024/10/11 01:19:24] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.11341428756713867\n",
      "[2024/10/11 01:19:24] ppocr DEBUG: cls num  : 1, elapsed : 0.013062238693237305\n",
      "[2024/10/11 01:19:24] ppocr DEBUG: rec_res num  : 1, elapsed : 0.06955981254577637\n",
      "data/train/images/002214_jpg.rf.740e8cb0355d34a45129b454ed0485cc.jpg [[67.0, 167.0], [337.0, 145.0], [345.0, 247.0], [75.0, 269.0]] ('MMR·7781', 0.9663845300674438) Duration: 225.5253791809082 ms\n",
      "[2024/10/11 01:19:24] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.11467123031616211\n",
      "[2024/10/11 01:19:24] ppocr DEBUG: cls num  : 1, elapsed : 0.013272285461425781\n",
      "[2024/10/11 01:19:24] ppocr DEBUG: rec_res num  : 1, elapsed : 0.06923079490661621\n",
      "data/train/images/001550_jpg.rf.549d4f6681430f99813e606cbf7932be.jpg [[158.0, 197.0], [257.0, 188.0], [260.0, 221.0], [161.0, 230.0]] ('8Z1905W', 0.6575247049331665) Duration: 228.71136665344238 ms\n",
      "[2024/10/11 01:19:24] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.12466597557067871\n",
      "[2024/10/11 01:19:24] ppocr DEBUG: cls num  : 2, elapsed : 0.01585531234741211\n",
      "[2024/10/11 01:19:25] ppocr DEBUG: rec_res num  : 2, elapsed : 0.11517548561096191\n",
      "data/train/images/000108_jpg.rf.d12cc4990f5f91962cb92057b0859a56.jpg [[177.0, 191.0], [248.0, 197.0], [246.0, 225.0], [175.0, 219.0]] ('MJA2610', 0.9258183240890503) Duration: 482.2537899017334 ms\n",
      "[2024/10/11 01:19:25] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.10729527473449707\n",
      "[2024/10/11 01:19:25] ppocr DEBUG: cls num  : 2, elapsed : 0.013491153717041016\n",
      "[2024/10/11 01:19:25] ppocr DEBUG: rec_res num  : 2, elapsed : 0.10753273963928223\n",
      "data/train/images/001362_jpg.rf.6ccee9a42a03e3edcafedc07780c01d7.jpg [[101.0, 200.0], [310.0, 183.0], [314.0, 229.0], [105.0, 246.0]] ('AEH-5601', 0.9616364240646362) Duration: 415.210485458374 ms\n",
      "[2024/10/11 01:19:25] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.10733532905578613\n",
      "[2024/10/11 01:19:25] ppocr DEBUG: cls num  : 1, elapsed : 0.012547492980957031\n",
      "[2024/10/11 01:19:25] ppocr DEBUG: rec_res num  : 1, elapsed : 0.0662088394165039\n",
      "data/train/images/000794_jpg.rf.cfb9e2e444253b0e8a4de3f0af2fc03d.jpg [[153.0, 183.0], [274.0, 195.0], [270.0, 235.0], [149.0, 223.0]] ('MRA·0136', 0.9240821003913879) Duration: 244.99821662902832 ms\n",
      "[2024/10/11 01:19:25] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.10068464279174805\n",
      "[2024/10/11 01:19:25] ppocr DEBUG: cls num  : 1, elapsed : 0.011351823806762695\n",
      "[2024/10/11 01:19:25] ppocr DEBUG: rec_res num  : 1, elapsed : 0.06063389778137207\n",
      "data/train/images/001494_jpg.rf.676d309ec292c112a880bd5a6ff62fb1.jpg [[177.0, 198.0], [235.0, 198.0], [235.0, 219.0], [177.0, 219.0]] ('BCN8757', 0.9574796557426453) Duration: 179.39996719360352 ms\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.10377097129821777\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: cls num  : 2, elapsed : 0.012689352035522461\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: rec_res num  : 2, elapsed : 0.1023716926574707\n",
      "data/train/images/001712_jpg.rf.568145cc5933f3c27e29d450780b3da8.jpg [[189.0, 200.0], [227.0, 200.0], [227.0, 215.0], [189.0, 215.0]] ('MRY-0890', 0.8842868208885193) Duration: 235.6555461883545 ms\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.09842038154602051\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: cls num  : 1, elapsed : 0.011228561401367188\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: rec_res num  : 1, elapsed : 0.05958676338195801\n",
      "data/train/images/002480_jpg.rf.8a679806a0421113a2d9b63f26850e2d.jpg [[171.0, 181.0], [252.0, 192.0], [246.0, 235.0], [165.0, 224.0]] ('BQS8910', 0.8809787631034851) Duration: 196.0141658782959 ms\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.10099625587463379\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: cls num  : 2, elapsed : 0.012608528137207031\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: rec_res num  : 2, elapsed : 0.10215497016906738\n",
      "data/train/images/001730_jpg.rf.882f3910f4fb593767cc633fd8efe98f.jpg [[145.0, 166.0], [284.0, 162.0], [285.0, 190.0], [146.0, 194.0]] ('台海省', 0.8204895853996277) Duration: 247.9536533355713 ms\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.0994877815246582\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: cls num  : 1, elapsed : 0.011038064956665039\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: rec_res num  : 1, elapsed : 0.060658931732177734\n",
      "data/train/images/001417_jpg.rf.7a61f93b41291e1b1d683812b87ef6fd.jpg [[120.0, 196.0], [296.0, 199.0], [295.0, 248.0], [119.0, 245.0]] ('995-PPX', 0.9009624123573303) Duration: 225.65555572509766 ms\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.10075116157531738\n",
      "[2024/10/11 01:19:26] ppocr DEBUG: cls num  : 3, elapsed : 0.017055034637451172\n",
      "[2024/10/11 01:19:27] ppocr DEBUG: rec_res num  : 3, elapsed : 0.16350793838500977\n",
      "data/train/images/001022_jpg.rf.b4b9154e7af05bff978389a635d3cb62.jpg 'NoneType' object is not subscriptable\n",
      "[2024/10/11 01:19:27] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.1147613525390625\n",
      "[2024/10/11 01:19:27] ppocr DEBUG: cls num  : 2, elapsed : 0.014628171920776367\n",
      "[2024/10/11 01:19:27] ppocr DEBUG: rec_res num  : 2, elapsed : 0.11619949340820312\n",
      "data/train/images/001117_jpg.rf.0828837a703a1fb4d22c6f7b09df6494.jpg [[126.0, 171.0], [299.0, 190.0], [293.0, 245.0], [120.0, 226.0]] ('NFD·9380', 0.9413707852363586) Duration: 289.8106575012207 ms\n",
      "[2024/10/11 01:19:27] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.11384224891662598\n",
      "[2024/10/11 01:19:27] ppocr DEBUG: cls num  : 2, elapsed : 0.015974044799804688\n",
      "[2024/10/11 01:19:27] ppocr DEBUG: rec_res num  : 2, elapsed : 0.11519408226013184\n",
      "data/train/images/000093_jpg.rf.365532a1c826ead3d0dcd6aa69a420ad.jpg [[167.0, 188.0], [255.0, 199.0], [251.0, 228.0], [163.0, 217.0]] ('NDE:0539', 0.9337817430496216) Duration: 276.317834854126 ms\n",
      "[2024/10/11 01:19:27] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.10949206352233887\n",
      "[2024/10/11 01:19:27] ppocr DEBUG: cls num  : 1, elapsed : 0.01217198371887207\n",
      "[2024/10/11 01:19:27] ppocr DEBUG: rec_res num  : 1, elapsed : 0.0605769157409668\n",
      "data/train/images/003131_jpg.rf.8471b038b9732dbbe9a2bdaacedb62e1.jpg [[80.0, 211.0], [125.0, 209.0], [126.0, 227.0], [81.0, 229.0]] ('ADV-1316', 0.9729009866714478) Duration: 243.62444877624512 ms\n",
      "[2024/10/11 01:19:28] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.09860897064208984\n",
      "[2024/10/11 01:19:28] ppocr DEBUG: cls num  : 1, elapsed : 0.011081695556640625\n",
      "[2024/10/11 01:19:28] ppocr DEBUG: rec_res num  : 1, elapsed : 0.05950617790222168\n",
      "data/train/images/000438_jpg.rf.469f1e8d132a26404cc376e70fa13962.jpg [[120.0, 186.0], [297.0, 183.0], [298.0, 233.0], [121.0, 236.0]] ('MQS-3155', 0.934528112411499) Duration: 227.39887237548828 ms\n",
      "[2024/10/11 01:19:28] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.10227251052856445\n",
      "[2024/10/11 01:19:28] ppocr DEBUG: cls num  : 4, elapsed : 0.018657684326171875\n",
      "[2024/10/11 01:19:28] ppocr DEBUG: rec_res num  : 4, elapsed : 0.20961427688598633\n",
      "data/train/images/001040_jpg.rf.11854e2901da2b14397cb2fb84186f1f.jpg [[138.0, 185.0], [288.0, 187.0], [287.0, 237.0], [137.0, 235.0]] ('MJL·8970', 0.96202552318573) Duration: 345.8573818206787 ms\n",
      "[2024/10/11 01:19:28] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.1148221492767334\n",
      "[2024/10/11 01:19:28] ppocr DEBUG: cls num  : 3, elapsed : 0.016318559646606445\n",
      "[2024/10/11 01:19:29] ppocr DEBUG: rec_res num  : 3, elapsed : 0.6323387622833252\n",
      "data/train/images/001463_jpg.rf.f64de49964ebb9f6fdebfcf2ddacb7b2.jpg [[198.0, 109.0], [213.0, 109.0], [213.0, 117.0], [198.0, 117.0]] ('417', 0.5876335501670837) Duration: 772.4435329437256 ms\n",
      "[2024/10/11 01:19:29] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.1114194393157959\n",
      "[2024/10/11 01:19:29] ppocr DEBUG: cls num  : 1, elapsed : 0.012925386428833008\n",
      "[2024/10/11 01:19:29] ppocr DEBUG: rec_res num  : 1, elapsed : 0.06884407997131348\n",
      "data/train/images/003052_jpg.rf.6042e59f24bafe1a78782b884b2c61e3.jpg [[185.0, 201.0], [234.0, 199.0], [235.0, 222.0], [186.0, 224.0]] ('7333YJ', 0.9946405291557312) Duration: 200.34313201904297 ms\n",
      "[2024/10/11 01:19:29] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.11258935928344727\n",
      "[2024/10/11 01:19:29] ppocr DEBUG: cls num  : 2, elapsed : 0.014880895614624023\n",
      "[2024/10/11 01:19:29] ppocr DEBUG: rec_res num  : 2, elapsed : 0.22362136840820312\n",
      "data/train/images/000910_jpg.rf.7486229721452aacc132479e9bb903d3.jpg [[142.0, 187.0], [280.0, 189.0], [279.0, 231.0], [141.0, 229.0]] ('MKM-6268', 0.8704283237457275) Duration: 359.9226474761963 ms\n",
      "[2024/10/11 01:19:29] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.11282062530517578\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: cls num  : 2, elapsed : 0.0195920467376709\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: rec_res num  : 2, elapsed : 0.10524225234985352\n",
      "data/train/images/000369_jpg.rf.c28a7ee38c92b5eaf182d04ca2ded282.jpg [[110.0, 191.0], [301.0, 181.0], [304.0, 230.0], [113.0, 240.0]] ('MEW-0777', 0.9612023830413818) Duration: 246.78325653076172 ms\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.10017824172973633\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: cls num  : 1, elapsed : 0.011441230773925781\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: rec_res num  : 1, elapsed : 0.07016611099243164\n",
      "data/train/images/001771_jpg.rf.48e7bd083efb908b96e62d38de5422e3.jpg [[103.0, 194.0], [311.0, 181.0], [315.0, 240.0], [107.0, 253.0]] ('512CAV', 0.9833731651306152) Duration: 188.68207931518555 ms\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.09926486015319824\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: cls num  : 1, elapsed : 0.01108407974243164\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: rec_res num  : 1, elapsed : 0.06055426597595215\n",
      "data/train/images/003287_jpg.rf.e49078729bc4cb75d3cc2981ed81afda.jpg [[161.0, 190.0], [257.0, 190.0], [257.0, 227.0], [161.0, 227.0]] ('AMY-6772', 0.9009155035018921) Duration: 178.2972812652588 ms\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.09926366806030273\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: cls num  : 1, elapsed : 0.01137995719909668\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: rec_res num  : 1, elapsed : 0.06084108352661133\n",
      "data/train/images/001955_jpg.rf.fc35f0a6269585751f53a3aebfbf398e.jpg [[171.0, 200.0], [241.0, 196.0], [242.0, 218.0], [172.0, 222.0]] ('RCR·9507', 0.9259630441665649) Duration: 178.1625747680664 ms\n",
      "[2024/10/11 01:19:30] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.09965157508850098\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m dir_list:\n\u001b[1;32m      2\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH, file)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mrec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mrec\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrec\u001b[39m(path):\n\u001b[1;32m      3\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m----> 4\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         text \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Extract the text\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/paddleocr/paddleocr.py:729\u001b[0m, in \u001b[0;36mPaddleOCR.ocr\u001b[0;34m(self, img, det, rec, cls, bin, inv, alpha_color, slice)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(imgs):\n\u001b[1;32m    728\u001b[0m     img \u001b[38;5;241m=\u001b[39m preprocess_image(img)\n\u001b[0;32m--> 729\u001b[0m     dt_boxes, rec_res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dt_boxes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rec_res:\n\u001b[1;32m    731\u001b[0m         ocr_res\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/paddleocr/tools/infer/predict_system.py:134\u001b[0m, in \u001b[0;36mTextSystem.__call__\u001b[0;34m(self, img, cls, slice)\u001b[0m\n\u001b[1;32m    132\u001b[0m     img_crop_list\u001b[38;5;241m.\u001b[39mappend(img_crop)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_angle_cls \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[0;32m--> 134\u001b[0m     img_crop_list, angle_list, elapse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_crop_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     time_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m elapse\n\u001b[1;32m    136\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls num  : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, elapsed : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(img_crop_list), elapse)\n\u001b[1;32m    138\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/paddleocr/tools/infer/predict_cls.py:105\u001b[0m, in \u001b[0;36mTextClassifier.__call__\u001b[0;34m(self, img_list)\u001b[0m\n\u001b[1;32m    103\u001b[0m     norm_img_batch\u001b[38;5;241m.\u001b[39mappend(norm_img)\n\u001b[1;32m    104\u001b[0m norm_img_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(norm_img_batch)\n\u001b[0;32m--> 105\u001b[0m norm_img_batch \u001b[38;5;241m=\u001b[39m \u001b[43mnorm_img_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_onnx:\n\u001b[1;32m    108\u001b[0m     input_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file in dir_list:\n",
    "    path = os.path.join(BASE_PATH, file)\n",
    "    rec(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca553961-434b-45e0-9b55-dc8ffcef0202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 01:19:49.708400: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-11 01:19:55.304916: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"PawanKrGunjan/license_plate_recognizer\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"PawanKrGunjan/license_plate_recognizer\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9890013-ad33-4fe0-8a97-f77f140d23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trocr_rec(path):\n",
    "    start_time = time()\n",
    "    try:\n",
    "        image = Image.open(path)\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        end_time = time()\n",
    "        print(f\"Image: {path}, Text: {generated_text}, Duration: {(end_time - start_time) * 1000:.2f} ms\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb2e2b4-37a4-45ef-8946-93576e8a7fa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: data/train/images/001074_jpg.rf.6f6406b7c8f5dd3ede891476267564eb.jpg, Text: OD 04W 5077, Duration: 3129.93 ms\n",
      "Image: data/train/images/001725_jpg.rf.9b6b05a541c7a6e6f1490d61e2b51a75.jpg, Text: OD 09R 9477, Duration: 242.10 ms\n",
      "Image: data/train/images/000883_jpg.rf.e84cb0ec653ba48ac6408013725d42c0.jpg, Text: TN 19N 7106, Duration: 235.39 ms\n",
      "Image: data/train/images/000472_jpg.rf.ff91707fe85e7cc4ba68251391042ec0.jpg, Text: OD 22BZ 2476, Duration: 263.88 ms\n",
      "Image: data/train/images/001604_jpg.rf.f859d1efd3834937199a6aa9b0687cb7.jpg, Text: WB 98J 5878, Duration: 233.09 ms\n",
      "Image: data/train/images/001286_jpg.rf.c3fc67b0c9cdaa6a04b9e7512bc7e58f.jpg, Text: HR 17 7267, Duration: 231.52 ms\n",
      "Image: data/train/images/000699_jpg.rf.af4122f2ffc9ec8d9d01c61566a951b7.jpg, Text: TN X 9579, Duration: 232.63 ms\n",
      "Image: data/train/images/002875_jpg.rf.e56e9f1378124cc8c413a3efc59922dc.jpg, Text: OD 09M 8757, Duration: 235.36 ms\n",
      "Image: data/train/images/001253_jpg.rf.d55b476c01d703291f23dcaec06155c3.jpg, Text: TN 77 7477, Duration: 232.59 ms\n",
      "Image: data/train/images/002788_jpg.rf.7501d546400def703ffdaa16bf09f4fb.jpg, Text: TN 99N 7DF, Duration: 261.77 ms\n",
      "Image: data/train/images/000925_jpg.rf.c9fbb218dcb717cd15279007b5e22a32.jpg, Text: TN 77W 7546, Duration: 233.72 ms\n",
      "Image: data/train/images/002924_jpg.rf.d3a156a0395fb699ae862c2317e57cfd.jpg, Text: OD 09R 9477, Duration: 235.80 ms\n",
      "Image: data/train/images/002221_jpg.rf.e5eacdb318cd52f2e28a866e5dce2f1b.jpg, Text: OD 16S 9191, Duration: 232.77 ms\n",
      "Image: data/train/images/000219_jpg.rf.4dd8a82ad8381962a3ad320ad8839484.jpg, Text: OD 27D 8578, Duration: 232.12 ms\n",
      "Image: data/train/images/002930_jpg.rf.cc39e17d9e1a11803c40c35029c3336f.jpg, Text: OD 04R 9477, Duration: 230.45 ms\n",
      "Image: data/train/images/000613_jpg.rf.6f4e10f07855265c25af78fc84f6eb6f.jpg, Text: TN 15N 4784, Duration: 231.65 ms\n",
      "Image: data/train/images/002214_jpg.rf.740e8cb0355d34a45129b454ed0485cc.jpg, Text: TN 17781, Duration: 205.38 ms\n",
      "Image: data/train/images/001550_jpg.rf.549d4f6681430f99813e606cbf7932be.jpg, Text: OD 04R 3598, Duration: 232.90 ms\n",
      "Image: data/train/images/000108_jpg.rf.d12cc4990f5f91962cb92057b0859a56.jpg, Text: OD 11Y 3191, Duration: 232.95 ms\n",
      "Image: data/train/images/001362_jpg.rf.6ccee9a42a03e3edcafedc07780c01d7.jpg, Text: OD 05N 5417, Duration: 326.13 ms\n",
      "Image: data/train/images/000794_jpg.rf.cfb9e2e444253b0e8a4de3f0af2fc03d.jpg, Text: OD 09R 4077, Duration: 287.07 ms\n",
      "Image: data/train/images/001494_jpg.rf.676d309ec292c112a880bd5a6ff62fb1.jpg, Text: OD 04R 9455, Duration: 278.59 ms\n",
      "Image: data/train/images/001712_jpg.rf.568145cc5933f3c27e29d450780b3da8.jpg, Text: OD 11R 9477, Duration: 268.85 ms\n",
      "Image: data/train/images/002480_jpg.rf.8a679806a0421113a2d9b63f26850e2d.jpg, Text: OD 04R 2166, Duration: 266.61 ms\n",
      "Image: data/train/images/001730_jpg.rf.882f3910f4fb593767cc633fd8efe98f.jpg, Text: OD 14L 5191, Duration: 292.18 ms\n",
      "Image: data/train/images/001417_jpg.rf.7a61f93b41291e1b1d683812b87ef6fd.jpg, Text: OR 54X 7559, Duration: 243.58 ms\n",
      "Image: data/train/images/001022_jpg.rf.b4b9154e7af05bff978389a635d3cb62.jpg, Text: TS 06UD 0660, Duration: 274.69 ms\n",
      "Image: data/train/images/001117_jpg.rf.0828837a703a1fb4d22c6f7b09df6494.jpg, Text: OD 0980, Duration: 199.72 ms\n",
      "Image: data/train/images/000093_jpg.rf.365532a1c826ead3d0dcd6aa69a420ad.jpg, Text: OD 09N 9477, Duration: 268.26 ms\n",
      "Image: data/train/images/003131_jpg.rf.8471b038b9732dbbe9a2bdaacedb62e1.jpg, Text: OD 04R 8477, Duration: 267.57 ms\n",
      "Image: data/train/images/000438_jpg.rf.469f1e8d132a26404cc376e70fa13962.jpg, Text: TS 14N 8166, Duration: 270.81 ms\n",
      "Image: data/train/images/001040_jpg.rf.11854e2901da2b14397cb2fb84186f1f.jpg, Text: WB 07W 1176, Duration: 266.05 ms\n",
      "Image: data/train/images/001463_jpg.rf.f64de49964ebb9f6fdebfcf2ddacb7b2.jpg, Text: WB 09Z 8282, Duration: 265.68 ms\n",
      "Image: data/train/images/003052_jpg.rf.6042e59f24bafe1a78782b884b2c61e3.jpg, Text: OD 04R 9166, Duration: 298.81 ms\n",
      "Image: data/train/images/000910_jpg.rf.7486229721452aacc132479e9bb903d3.jpg, Text: OD 09R 3299, Duration: 267.91 ms\n",
      "Image: data/train/images/000369_jpg.rf.c28a7ee38c92b5eaf182d04ca2ded282.jpg, Text: WB 04E 0771, Duration: 266.28 ms\n",
      "Image: data/train/images/001771_jpg.rf.48e7bd083efb908b96e62d38de5422e3.jpg, Text: OD 33V 0215, Duration: 272.09 ms\n",
      "Image: data/train/images/003287_jpg.rf.e49078729bc4cb75d3cc2981ed81afda.jpg, Text: OD 04R 9303, Duration: 267.65 ms\n",
      "Image: data/train/images/001955_jpg.rf.fc35f0a6269585751f53a3aebfbf398e.jpg, Text: OD 77R 9477, Duration: 265.68 ms\n",
      "Image: data/train/images/002198_jpg.rf.d03e35ed5376ddefe93cb33f0ccb6ea5.jpg, Text: OD 04R 6477, Duration: 264.66 ms\n",
      "Image: data/train/images/003112_jpg.rf.cea277329f990695137690a77e045fbd.jpg, Text: OD 11B 5477, Duration: 264.35 ms\n",
      "Image: data/train/images/000045_jpg.rf.d4c91ddfe61b63cfccbfa65f0e33db1f.jpg, Text: OD 09R 9771, Duration: 267.76 ms\n",
      "Image: data/train/images/000694_jpg.rf.b6f05bd18e3bd7ed0158c5376779d7b6.jpg, Text: OD 14N 2520, Duration: 256.95 ms\n",
      "Image: data/train/images/000959_jpg.rf.0b188495d8c359c9867fd18f43c149a4.jpg, Text: OD 29N 6200, Duration: 240.36 ms\n",
      "Image: data/train/images/000243_jpg.rf.35207eb767246607a27fbdfdaa15b5e7.jpg, Text: OD 0417, Duration: 179.55 ms\n",
      "Image: data/train/images/000521_jpg.rf.fc81860c59eac2a6c99aaa79dc553e1e.jpg, Text: OD 05NAD 0155, Duration: 259.58 ms\n",
      "Image: data/train/images/002980_jpg.rf.550abf04fb3156d444fce5df33bc4508.jpg, Text: OD 05G 3166, Duration: 231.14 ms\n",
      "Image: data/train/images/002358_jpg.rf.315958fcda981064777371b2f042ba89.jpg, Text: RK 9578, Duration: 230.05 ms\n",
      "Image: data/train/images/000753_jpg.rf.7e4b136ea330031e1b56bb65ea3c4ee7.jpg, Text: RJ 9578, Duration: 200.97 ms\n",
      "Image: data/train/images/002326_jpg.rf.a94c7f948e393169d705e5a70492a712.jpg, Text: OD 09BE 1405, Duration: 231.21 ms\n",
      "Image: data/train/images/001408_jpg.rf.5f953c774d962545e75bbe64a39559c3.jpg, Text: OD 04W 9477, Duration: 232.14 ms\n",
      "Image: data/train/images/002018_jpg.rf.fe5e67418261236952ddfb6de41e79ed.jpg, Text: OD 09N 8954, Duration: 231.12 ms\n",
      "Image: data/train/images/002275_jpg.rf.295dfee6a1d4f1f42f7ef48d1e282eb7.jpg, Text: OD 02G 9166, Duration: 231.86 ms\n",
      "Image: data/train/images/002231_jpg.rf.af1fffacca7879d5876cc0bbc35c4d85.jpg, Text: OD 04R 9166, Duration: 230.86 ms\n",
      "Image: data/train/images/001313_jpg.rf.d314daf39e25ffe0b65fda93afdbe25a.jpg, Text: WB 16N 7167, Duration: 235.96 ms\n",
      "Image: data/train/images/001296_jpg.rf.8c51f96148885ce0bb92bc7b44be92df.jpg, Text: OD 09L 8170, Duration: 264.08 ms\n",
      "Image: data/train/images/001952_jpg.rf.63ef162c6bed18ed8cf8e777fb8c9dc6.jpg, Text: WB X 5477, Duration: 202.88 ms\n",
      "Image: data/train/images/002281_jpg.rf.4edb4f6f2b5d023d0be241f44ce68122.jpg, Text: OD 06R 9071, Duration: 259.54 ms\n",
      "Image: data/train/images/003007_jpg.rf.ba55feec9f97155decce88888c6b644c.jpg, Text: OD 04R 9477, Duration: 231.90 ms\n",
      "Image: data/train/images/001202_jpg.rf.ad4e41dc92b844f4df347409c9dd24d8.jpg, Text: OD 09W 3477, Duration: 234.85 ms\n",
      "Image: data/train/images/002064_jpg.rf.89fc39281db6d3d2f994b695b271a1c0.jpg, Text: OD 09W 0477, Duration: 233.07 ms\n",
      "Image: data/train/images/000105_jpg.rf.a333b7a330e1676b2316e1508438e645.jpg, Text: OD 11L 9191, Duration: 232.32 ms\n",
      "Image: data/train/images/001847_jpg.rf.9bee95b99cdc60fa81298f9d0e31405b.jpg, Text: OD 16R 9477, Duration: 234.19 ms\n",
      "Image: data/train/images/002685_jpg.rf.70c279e3c32f8dd6ac147ecedb6ab374.jpg, Text: OD 09N 5591, Duration: 232.57 ms\n",
      "Image: data/train/images/002550_jpg.rf.1ff4bc7b57b050ada0218534f151cdc7.jpg, Text: OD 09R 9477, Duration: 237.75 ms\n",
      "Image: data/train/images/000311_jpg.rf.cb0503ed56cfcf476455618f9a2a0c7d.jpg, Text: WBH 0170, Duration: 204.33 ms\n",
      "Image: data/train/images/003104_jpg.rf.49d92ecc050a5a5a60b872f8d863450b.jpg, Text: OD 04R 9578, Duration: 231.35 ms\n",
      "Image: data/train/images/000313_jpg.rf.50ae115e1945fc895201950546e7b1eb.jpg, Text: WB 06N 0646, Duration: 231.86 ms\n",
      "Image: data/train/images/002752_jpg.rf.f89e7774139a4bdd095abf23b105f940.jpg, Text: OD 09R 8477, Duration: 234.98 ms\n",
      "Image: data/train/images/002848_jpg.rf.79adb859898ff973aaeb24a3e0523df8.jpg, Text: OD 04R 3299, Duration: 236.68 ms\n",
      "Image: data/train/images/001459_jpg.rf.f7d63094afcfdab61ae99695f68dceb9.jpg, Text: OD 02BQ 2087, Duration: 260.50 ms\n",
      "Image: data/train/images/000270_jpg.rf.616ed4700278c98ceaf93036ec73e5bc.jpg, Text: CU 07WCU 0571, Duration: 260.44 ms\n",
      "Image: data/train/images/001105_jpg.rf.0542f36a578d5b0d4ea6e05be0473e6d.jpg, Text: TN 26W 3270, Duration: 233.85 ms\n",
      "Image: data/train/images/002617_jpg.rf.5e99f16f2342bd4e01093e96d5fe9312.jpg, Text: OD 09R 9477, Duration: 231.60 ms\n",
      "Image: data/train/images/003058_jpg.rf.1e543b179b4c331a75e2445b225549d0.jpg, Text: OD 09L 9598, Duration: 231.99 ms\n",
      "Image: data/train/images/000036_jpg.rf.115665d1252053abcde13a75b6076133.jpg, Text: OD 09R 5477, Duration: 231.48 ms\n",
      "Image: data/train/images/000402_jpg.rf.0bb555c634ea26bb72c38f3008cff96d.jpg, Text: OD 14M 6120, Duration: 232.49 ms\n",
      "Image: data/train/images/000661_jpg.rf.87688562b3c57ab798c1f237bbc93337.jpg, Text: OD 09, Duration: 233.20 ms\n",
      "Image: data/train/images/003151_jpg.rf.178c918743f72fe908cc06499e1efb59.jpg, Text: OD 09UM 0784, Duration: 233.11 ms\n",
      "Image: data/train/images/001524_jpg.rf.54a847d04d7ab0e599499768cfae7fc0.jpg, Text: ODH 0477, Duration: 203.20 ms\n",
      "Image: data/train/images/002607_jpg.rf.7ae3529f57dab39953025233b1fae9f4.jpg, Text: CG 09M 5581, Duration: 232.73 ms\n",
      "Image: data/train/images/001309_jpg.rf.0baba392a8c6b397f15a3dd2bb4ceff4.jpg, Text: WB 19E 9019, Duration: 231.33 ms\n",
      "Image: data/train/images/003011_jpg.rf.876e0b1d3861f78d702e87cc4953ae01.jpg, Text: OD 04R 3477, Duration: 234.83 ms\n",
      "Image: data/train/images/000800_jpg.rf.861bc2b6eab2b172136643a8d084ed19.jpg, Text: OD 06N 7478, Duration: 232.93 ms\n",
      "Image: data/train/images/000191_jpg.rf.21fdc62b7b11e2441c81022c33d0204e.jpg, Text: OD 09T 9477, Duration: 233.93 ms\n",
      "Image: data/train/images/001638_jpg.rf.df12f4ccfa7b7a1747bfa80bb049c869.jpg, Text: OD 09R 3299, Duration: 234.61 ms\n",
      "Image: data/train/images/000862_jpg.rf.c59aac60c88d24c1b2b5e7e27efa7a79.jpg, Text: OD 25V 0237, Duration: 252.55 ms\n",
      "Image: data/train/images/001832_jpg.rf.b53676464b97baecce10e08befb5651c.jpg, Text: OD 14R 8477, Duration: 254.17 ms\n",
      "Image: data/train/images/001505_jpg.rf.489031d4e26332c0a138addf78c875aa.jpg, Text: OD 09G 6679, Duration: 240.84 ms\n",
      "Image: data/train/images/000487_jpg.rf.99204a3ebdc955d37ab0399a2a15231a.jpg, Text: NZ 09Z 0285, Duration: 243.50 ms\n",
      "Image: data/train/images/002169_jpg.rf.150ef8310f5aef9b9da76d82f72bfb9d.jpg, Text: NL 8751, Duration: 239.91 ms\n",
      "Image: data/train/images/001162_jpg.rf.49b9b2aaf4b7cb1848278e97f80b126c.jpg, Text: OD 04R 9477, Duration: 238.68 ms\n",
      "Image: data/train/images/001176_jpg.rf.cb6677585610c5712aaa23ea66b30c26.jpg, Text: WB 52H 5592, Duration: 242.74 ms\n",
      "Image: data/train/images/002644_jpg.rf.c225262e87246df3c66747fd869d8b4b.jpg, Text: OD 67R 9477, Duration: 241.87 ms\n",
      "Image: data/train/images/000053_jpg.rf.972012a8fb42cecde1ec0d7c3f636d6d.jpg, Text: OD 04R 9404, Duration: 234.82 ms\n",
      "Image: data/train/images/001684_jpg.rf.718fb0408e76e8004e56304d866fd837.jpg, Text: OD 09R 9477, Duration: 241.48 ms\n",
      "Image: data/train/images/002562_jpg.rf.10fabd227fd7d53e06ea5ba1f4afb510.jpg, Text: OD 09R 8578, Duration: 240.00 ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m dir_list:\n\u001b[1;32m      2\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH, file)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrocr_rec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mtrocr_rec\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      4\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(path)\n\u001b[1;32m      5\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m processor(image, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mpixel_values\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time()\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/transformers/generation/utils.py:1795\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1787\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1788\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1789\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1790\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1791\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1792\u001b[0m     )\n\u001b[1;32m   1794\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 1795\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   1807\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1808\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1809\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1810\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1816\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1817\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/transformers/generation/utils.py:2654\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2651\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 2654\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2657\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2662\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:615\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[0;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m    611\u001b[0m         labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m    612\u001b[0m     )\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py:913\u001b[0m, in \u001b[0;36mTrOCRForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    910\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 913\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_projection(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    930\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py:673\u001b[0m, in \u001b[0;36mTrOCRDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    660\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    661\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    662\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    670\u001b[0m         use_cache,\n\u001b[1;32m    671\u001b[0m     )\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 673\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py:380\u001b[0m, in \u001b[0;36mTrOCRDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m    379\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    390\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py:203\u001b[0m, in \u001b[0;36mTrOCRAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    200\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_cross_attention:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# cross_attentions\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(key_value_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# reuse k, v, self_attention\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rtfa/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py:176\u001b[0m, in \u001b[0;36mTrOCRAttention._shape\u001b[0;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: torch\u001b[38;5;241m.\u001b[39mTensor, seq_len: \u001b[38;5;28mint\u001b[39m, bsz: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file in dir_list:\n",
    "    path = os.path.join(BASE_PATH, file)\n",
    "    trocr_rec(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62588b94-f4d0-4059-8f52-ffc0be2580f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4dd9164-c18c-4cab-9f32-fa649a724588",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"models/yolo11x.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec170a-ebe6-454f-8813-325a1d70d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.10 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.9 🚀 Python-3.10.13 torch-2.1.2+cu121 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=models/yolo11x.pt, data=datasets/data.yaml, epochs=150, time=None, patience=20, batch=12, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train12, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train12\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]                 \n",
      "  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]     \n",
      "  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      "  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]     \n",
      "  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      "  6                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
      "  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      "  8                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
      "  9                  -1  1   1476864  ultralytics.nn.modules.block.SPPF            [768, 768, 5]                 \n",
      " 10                  -1  2   3264768  ultralytics.nn.modules.block.C2PSA           [768, 768, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2   1700352  ultralytics.nn.modules.block.C3k2            [1536, 384, 2, True]          \n",
      " 17                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   5317632  ultralytics.nn.modules.block.C3k2            [1152, 768, 2, True]          \n",
      " 20                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
      " 23        [16, 19, 22]  1   3146707  ultralytics.nn.modules.head.Detect           [1, [384, 768, 768]]          \n",
      "YOLO11x summary: 631 layers, 56,874,931 parameters, 56,874,915 gradients, 195.4 GFLOPs\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train12', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/duc-softzone/intelligent-surveillance-system/src/plate-recog/src/datasets/train/labels.cache... 18816 images, 0 backgrounds, 0 corrupt: 100%|██████████| 18816/18816 [00:00<?, ?it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 329, len(boxes) = 25354. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/duc-softzone/intelligent-surveillance-system/src/plate-recog/src/datasets/valid/labels.cache... 497 images, 0 backgrounds, 0 corrupt: 100%|██████████| 497/497 [00:00<?, ?it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 6, len(boxes) = 633. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train12/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.00046875), 173 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train12\u001b[0m\n",
      "Starting training for 150 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/150      13.7G      0.782     0.5959      1.093         25        640: 100%|██████████| 1568/1568 [16:23<00:00,  1.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:14<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.981      0.972      0.986      0.822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/150      13.6G       0.77     0.4587      1.065         27        640: 100%|██████████| 1568/1568 [16:40<00:00,  1.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.977       0.96      0.985      0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/150      13.9G     0.8356     0.5301      1.112         23        640: 100%|██████████| 1568/1568 [16:10<00:00,  1.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.956      0.956      0.971      0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/150      13.8G     0.8516     0.5382      1.139         24        640: 100%|██████████| 1568/1568 [15:50<00:00,  1.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.973      0.964      0.983      0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/150      13.7G     0.7885      0.473      1.107         36        640: 100%|██████████| 1568/1568 [16:12<00:00,  1.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.982      0.954      0.985      0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/150      13.5G      0.742     0.4378      1.078         13        640: 100%|██████████| 1568/1568 [16:14<00:00,  1.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.977      0.968      0.989       0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/150      13.8G     0.7137     0.4124      1.063         23        640: 100%|██████████| 1568/1568 [15:57<00:00,  1.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.986      0.967       0.99      0.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/150      13.8G     0.6886     0.3968      1.045         23        640: 100%|██████████| 1568/1568 [16:07<00:00,  1.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.984      0.975      0.992      0.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      9/150      13.7G     0.6619     0.3788      1.032         28        640: 100%|██████████| 1568/1568 [16:12<00:00,  1.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.982      0.965       0.99      0.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     10/150      13.8G     0.6439     0.3611      1.024         25        640: 100%|██████████| 1568/1568 [15:44<00:00,  1.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.974      0.979      0.989       0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     11/150      13.5G     0.6312     0.3542      1.018         23        640: 100%|██████████| 1568/1568 [16:10<00:00,  1.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.982      0.976      0.989      0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     12/150      13.7G     0.6126     0.3419      1.003         21        640: 100%|██████████| 1568/1568 [16:19<00:00,  1.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633       0.98      0.978      0.991      0.878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     13/150      13.8G     0.5986     0.3355     0.9986         32        640: 100%|██████████| 1568/1568 [16:13<00:00,  1.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.981      0.981      0.993      0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     14/150      13.7G     0.5858     0.3276     0.9932         23        640: 100%|██████████| 1568/1568 [16:05<00:00,  1.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.986      0.978      0.992      0.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     15/150      13.8G     0.5804     0.3265     0.9912         27        640: 100%|██████████| 1568/1568 [15:56<00:00,  1.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.987      0.977      0.992      0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     16/150      13.8G     0.5664     0.3161     0.9831         23        640: 100%|██████████| 1568/1568 [16:04<00:00,  1.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.983      0.981      0.993      0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     17/150      13.8G     0.5536     0.3109     0.9796         29        640: 100%|██████████| 1568/1568 [16:04<00:00,  1.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633       0.98      0.983      0.993      0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     18/150      13.8G     0.5489     0.3067     0.9737         18        640: 100%|██████████| 1568/1568 [16:24<00:00,  1.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.986      0.978      0.993      0.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     19/150      13.8G     0.5425     0.3036     0.9709         32        640: 100%|██████████| 1568/1568 [16:12<00:00,  1.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.982      0.976      0.993      0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     20/150      13.8G      0.533     0.2978     0.9638         34        640: 100%|██████████| 1568/1568 [16:04<00:00,  1.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.988      0.976      0.993      0.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     21/150      13.6G     0.5222     0.2912     0.9608         25        640: 100%|██████████| 1568/1568 [16:07<00:00,  1.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.982      0.979      0.994      0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     22/150      13.8G     0.5173     0.2883     0.9552         37        640: 100%|██████████| 1568/1568 [16:14<00:00,  1.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.982      0.979      0.993       0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     23/150      13.8G      0.509     0.2853     0.9537         23        640: 100%|██████████| 1568/1568 [16:03<00:00,  1.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.983      0.979      0.993      0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     24/150      13.8G     0.5043     0.2824     0.9532         30        640: 100%|██████████| 1568/1568 [16:22<00:00,  1.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.982      0.979      0.994       0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     25/150      13.7G      0.491     0.2761     0.9457         29        640: 100%|██████████| 1568/1568 [15:37<00:00,  1.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.986      0.981      0.994      0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     26/150      13.6G     0.4863     0.2726     0.9429         38        640: 100%|██████████| 1568/1568 [16:09<00:00,  1.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.984      0.981      0.993       0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     27/150      13.7G     0.4806     0.2714     0.9409         26        640: 100%|██████████| 1568/1568 [16:18<00:00,  1.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:16<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.986      0.978      0.994      0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     28/150      13.7G     0.4732     0.2669     0.9381         32        640: 100%|██████████| 1568/1568 [16:04<00:00,  1.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.986      0.981      0.994      0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     29/150      13.8G     0.4639     0.2626     0.9339         24        640: 100%|██████████| 1568/1568 [16:00<00:00,  1.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.985      0.983      0.993      0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     30/150      13.7G      0.464      0.263     0.9294         20        640: 100%|██████████| 1568/1568 [16:24<00:00,  1.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.986      0.983      0.993      0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     31/150      13.6G     0.4593     0.2589     0.9287         28        640: 100%|██████████| 1568/1568 [16:12<00:00,  1.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:15<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.984      0.981      0.993      0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     32/150      13.8G     0.4616     0.2591     0.9303         25        640:  47%|████▋     | 736/1568 [07:27<08:26,  1.64it/s]"
     ]
    }
   ],
   "source": [
    "results = model.train(data=\"/home/duc-softzone/intelligent-surveillance-system/src/plate-recog/src/datasets/data.yaml\", epochs=150, batch=12, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb77e80-7cba-43ae-b1f1-bcec4fa18039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.9 🚀 Python-3.10.13 torch-2.1.2+cu121 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "YOLO11x summary (fused): 464 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/duc-softzone/intelligent-surveillance-system/src/plate-recog/src/datasets/valid/labels.cache... 497 images, 0 backgrounds, 0 corrupt: 100\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 6, len(boxes) = 633. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 42/42 [00:11<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        497        633      0.989      0.981      0.994      0.922\n",
      "Speed: 0.2ms preprocess, 14.7ms inference, 0.0ms loss, 3.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train72\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7a47f22158a0>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,      0.9983,\n",
       "             0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,\n",
       "             0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,\n",
       "             0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,      0.9983,     0.99664,     0.99664,     0.99664,     0.99664,     0.99664,     0.99664,     0.99664,     0.99664,     0.99664,     0.99664,     0.99499,     0.99499,     0.99499,     0.99336,     0.99336,\n",
       "            0.99336,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,\n",
       "            0.99192,     0.99192,     0.99192,     0.99192,     0.99042,     0.99042,     0.99042,     0.99042,     0.99042,     0.99042,     0.99042,     0.99042,     0.99042,     0.98885,     0.98885,     0.98418,     0.97802,     0.97802,       0.975,      0.9705,      0.9705,     0.96308,     0.96024,\n",
       "            0.96024,     0.96024,     0.96024,     0.90374,     0.84083,      0.7007,     0.56056,     0.42042,     0.28028,     0.14014,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.93601,     0.93601,     0.95147,     0.95751,     0.96349,     0.96476,     0.96767,     0.96803,     0.96841,     0.97024,     0.97125,     0.97175,     0.97226,     0.97328,     0.97382,     0.97455,     0.97511,     0.97591,     0.97533,     0.97442,     0.97478,     0.97512,     0.97534,\n",
       "            0.97557,     0.97579,     0.97551,      0.9751,     0.97588,     0.97598,     0.97608,     0.97619,     0.97629,     0.97639,     0.97649,     0.97659,     0.97667,     0.97676,     0.97685,     0.97694,     0.97703,     0.97712,     0.97721,      0.9773,     0.97737,     0.97742,     0.97748,\n",
       "            0.97754,      0.9776,     0.97766,     0.97772,     0.97778,     0.97784,     0.97789,     0.97795,     0.97801,     0.97807,     0.97812,     0.97817,     0.97822,     0.97828,     0.97833,     0.97838,     0.97843,     0.97848,     0.97853,     0.97858,     0.97863,     0.97868,     0.97873,\n",
       "            0.97878,     0.97883,     0.97885,     0.97883,     0.97881,     0.97879,     0.97878,     0.97876,     0.97874,     0.97872,      0.9787,     0.97869,     0.97867,     0.97865,     0.97863,     0.97862,      0.9786,     0.97858,     0.97856,     0.97854,     0.97853,     0.97851,     0.97849,\n",
       "            0.97847,     0.97845,     0.97844,     0.97842,      0.9784,     0.97838,     0.97836,     0.97835,     0.97833,     0.97831,     0.97829,     0.97827,     0.97826,     0.97824,     0.97822,      0.9782,     0.97819,     0.97817,     0.97815,     0.97813,     0.97811,      0.9781,     0.97808,\n",
       "            0.97806,     0.97808,     0.97811,     0.97814,     0.97816,     0.97819,     0.97822,     0.97825,     0.97828,      0.9783,     0.97833,     0.97836,     0.97839,     0.97842,     0.97845,     0.97847,      0.9785,     0.97853,     0.97856,     0.97859,     0.97861,     0.97864,     0.97867,\n",
       "             0.9787,     0.97873,     0.97876,     0.97878,     0.97881,     0.97897,     0.97921,     0.97946,     0.97961,     0.97964,     0.97968,     0.97971,     0.97975,     0.97978,     0.97982,     0.97985,     0.97988,     0.97992,     0.97995,     0.97999,     0.98002,     0.98006,     0.98009,\n",
       "            0.98013,     0.98016,     0.98019,     0.98023,     0.98026,      0.9803,     0.98033,     0.98036,     0.98034,     0.98032,      0.9803,     0.98028,     0.98026,     0.98024,     0.98022,      0.9802,     0.98018,     0.98016,     0.98014,     0.98012,      0.9801,     0.98008,     0.98006,\n",
       "            0.98004,     0.98002,        0.98,     0.97998,     0.97996,     0.97994,     0.97992,      0.9799,     0.97988,     0.97986,     0.97984,     0.97982,      0.9798,     0.97978,     0.97976,     0.97974,     0.97972,      0.9797,     0.97968,     0.97966,     0.97964,     0.97962,      0.9796,\n",
       "            0.97958,     0.97956,     0.97957,     0.97959,      0.9796,     0.97961,     0.97962,     0.97964,     0.97965,     0.97966,     0.97967,     0.97969,      0.9797,     0.97971,     0.97972,     0.97974,     0.97975,     0.97976,     0.97977,     0.97979,      0.9798,     0.97981,     0.97982,\n",
       "            0.97983,     0.97985,     0.97986,     0.97987,     0.97988,      0.9799,     0.97991,     0.97992,     0.97993,     0.97995,     0.97996,     0.97997,     0.97998,        0.98,     0.98001,     0.98002,     0.98003,     0.98005,     0.98006,     0.98007,     0.98008,      0.9801,     0.98011,\n",
       "            0.98012,     0.98013,     0.98015,     0.98016,     0.98017,     0.98018,      0.9802,     0.98021,     0.98022,     0.98023,     0.98024,     0.98026,     0.98027,     0.98028,     0.98029,     0.98031,     0.98032,     0.98033,     0.98037,     0.98041,     0.98044,     0.98048,     0.98051,\n",
       "            0.98055,     0.98058,     0.98062,     0.98066,     0.98069,     0.98073,     0.98076,      0.9808,     0.98084,     0.98087,     0.98091,     0.98094,     0.98098,     0.98101,     0.98105,     0.98109,     0.98109,     0.98106,     0.98103,       0.981,     0.98097,     0.98094,     0.98091,\n",
       "            0.98088,     0.98085,     0.98082,     0.98079,     0.98076,     0.98073,      0.9807,     0.98067,     0.98064,     0.98061,     0.98059,     0.98056,     0.98053,      0.9805,     0.98047,     0.98044,     0.98041,     0.98038,     0.98035,     0.98032,     0.98039,     0.98068,     0.98097,\n",
       "            0.98124,      0.9815,     0.98176,     0.98185,     0.98186,     0.98187,     0.98188,     0.98189,      0.9819,     0.98191,     0.98192,     0.98193,     0.98194,     0.98195,     0.98196,     0.98197,     0.98198,     0.98199,       0.982,     0.98201,     0.98203,     0.98204,     0.98205,\n",
       "            0.98206,     0.98207,     0.98208,     0.98209,      0.9821,     0.98211,     0.98212,     0.98213,     0.98214,     0.98215,     0.98216,     0.98217,     0.98218,     0.98219,      0.9822,     0.98221,     0.98222,     0.98223,     0.98224,     0.98225,     0.98226,     0.98227,     0.98228,\n",
       "            0.98229,      0.9823,     0.98231,     0.98232,     0.98233,     0.98234,     0.98235,     0.98236,     0.98237,     0.98238,     0.98239,      0.9824,     0.98241,     0.98242,     0.98243,     0.98244,     0.98245,     0.98246,     0.98247,     0.98248,     0.98249,      0.9825,     0.98251,\n",
       "            0.98252,     0.98253,     0.98254,     0.98255,     0.98256,     0.98257,     0.98258,     0.98259,      0.9826,     0.98261,     0.98262,     0.98263,     0.98263,     0.98264,     0.98265,     0.98266,     0.98266,     0.98267,     0.98268,     0.98269,     0.98269,      0.9827,     0.98271,\n",
       "            0.98271,     0.98272,     0.98273,     0.98274,     0.98274,     0.98275,     0.98276,     0.98276,     0.98277,     0.98278,     0.98279,     0.98279,      0.9828,     0.98281,     0.98282,     0.98282,     0.98283,     0.98284,     0.98284,     0.98285,     0.98286,     0.98287,     0.98287,\n",
       "            0.98288,     0.98289,     0.98289,      0.9829,     0.98291,     0.98292,     0.98292,     0.98293,     0.98294,     0.98294,     0.98295,     0.98296,     0.98297,     0.98297,     0.98298,     0.98299,       0.983,       0.983,     0.98301,     0.98302,     0.98302,     0.98303,     0.98304,\n",
       "            0.98305,     0.98305,     0.98306,     0.98307,     0.98307,     0.98308,     0.98309,      0.9831,      0.9831,     0.98311,     0.98312,     0.98313,     0.98313,     0.98314,     0.98315,     0.98315,     0.98316,     0.98317,     0.98318,     0.98318,     0.98319,      0.9832,      0.9832,\n",
       "            0.98321,     0.98322,     0.98323,     0.98323,     0.98324,     0.98325,     0.98326,     0.98326,     0.98327,     0.98328,     0.98328,     0.98329,      0.9833,     0.98331,     0.98331,     0.98332,     0.98333,     0.98333,     0.98334,     0.98335,     0.98336,     0.98336,     0.98337,\n",
       "            0.98338,     0.98338,     0.98339,      0.9834,     0.98332,     0.98325,     0.98318,      0.9831,     0.98303,     0.98296,     0.98288,     0.98281,     0.98273,     0.98266,      0.9826,     0.98261,     0.98262,     0.98263,     0.98265,     0.98266,     0.98267,     0.98269,      0.9827,\n",
       "            0.98271,     0.98272,     0.98274,     0.98275,     0.98276,     0.98277,     0.98279,      0.9828,     0.98281,     0.98283,     0.98284,     0.98285,     0.98286,     0.98288,     0.98289,      0.9829,     0.98291,     0.98293,     0.98294,     0.98295,     0.98297,     0.98298,     0.98299,\n",
       "              0.983,     0.98302,     0.98303,     0.98304,     0.98305,     0.98307,     0.98308,     0.98309,     0.98311,     0.98312,     0.98313,     0.98314,     0.98316,     0.98317,     0.98318,     0.98319,     0.98321,     0.98322,     0.98323,     0.98325,     0.98326,     0.98327,     0.98328,\n",
       "             0.9833,     0.98331,     0.98332,     0.98333,     0.98335,     0.98336,     0.98337,     0.98339,     0.98341,     0.98343,     0.98345,     0.98347,     0.98349,     0.98351,     0.98353,     0.98355,     0.98357,     0.98359,     0.98361,     0.98363,     0.98365,     0.98367,     0.98369,\n",
       "             0.9837,     0.98372,     0.98374,     0.98376,     0.98378,      0.9838,     0.98382,     0.98384,     0.98386,     0.98388,      0.9839,     0.98392,     0.98394,     0.98396,     0.98398,       0.984,     0.98402,     0.98404,     0.98406,     0.98408,      0.9841,     0.98411,     0.98413,\n",
       "            0.98415,     0.98416,     0.98417,     0.98418,     0.98419,      0.9842,     0.98421,     0.98422,     0.98423,     0.98424,     0.98425,     0.98426,     0.98427,     0.98428,     0.98429,      0.9843,     0.98431,     0.98432,     0.98433,     0.98434,     0.98435,     0.98436,     0.98437,\n",
       "            0.98438,     0.98439,      0.9844,     0.98441,     0.98442,     0.98443,     0.98444,     0.98445,     0.98446,     0.98447,     0.98448,     0.98449,      0.9845,     0.98451,     0.98452,     0.98453,     0.98454,     0.98455,     0.98456,     0.98457,     0.98458,     0.98459,      0.9846,\n",
       "            0.98461,     0.98462,     0.98463,     0.98464,     0.98465,     0.98466,     0.98467,     0.98468,     0.98469,      0.9847,     0.98471,     0.98472,     0.98473,     0.98474,     0.98475,     0.98476,     0.98477,     0.98478,     0.98479,      0.9848,     0.98481,     0.98482,     0.98483,\n",
       "            0.98484,     0.98485,     0.98486,     0.98487,     0.98488,     0.98489,      0.9849,     0.98491,     0.98492,     0.98493,     0.98492,     0.98491,     0.98491,      0.9849,     0.98489,     0.98488,     0.98487,     0.98486,     0.98485,     0.98484,     0.98483,     0.98482,     0.98481,\n",
       "             0.9848,     0.98479,     0.98478,     0.98477,     0.98476,     0.98475,     0.98475,     0.98474,     0.98473,     0.98472,     0.98471,      0.9847,     0.98469,     0.98468,     0.98467,     0.98466,     0.98465,     0.98464,     0.98463,     0.98462,     0.98461,      0.9846,     0.98459,\n",
       "            0.98458,     0.98458,     0.98457,     0.98456,     0.98455,     0.98454,     0.98453,     0.98452,     0.98451,      0.9845,     0.98449,     0.98448,     0.98447,     0.98446,     0.98445,     0.98444,     0.98443,     0.98442,     0.98441,     0.98441,      0.9844,     0.98439,     0.98438,\n",
       "            0.98437,     0.98436,     0.98435,     0.98434,     0.98433,     0.98432,     0.98431,      0.9843,     0.98429,     0.98428,     0.98427,     0.98426,     0.98425,     0.98424,     0.98424,     0.98423,     0.98422,     0.98421,      0.9842,     0.98419,     0.98418,     0.98417,     0.98416,\n",
       "            0.98415,     0.98414,     0.98413,     0.98416,     0.98423,     0.98429,     0.98436,     0.98443,     0.98449,     0.98456,     0.98462,     0.98469,     0.98476,     0.98482,     0.98489,     0.98481,     0.98466,     0.98452,     0.98437,     0.98423,     0.98408,     0.98392,     0.98376,\n",
       "            0.98359,     0.98343,     0.98324,     0.98292,      0.9826,     0.98247,     0.98244,     0.98241,     0.98238,     0.98235,     0.98232,     0.98229,     0.98227,     0.98224,     0.98221,     0.98218,     0.98215,     0.98212,     0.98209,     0.98207,     0.98204,     0.98201,     0.98198,\n",
       "            0.98195,     0.98192,     0.98189,     0.98187,     0.98184,     0.98181,     0.98178,     0.98175,     0.98172,     0.98169,      0.9816,     0.98128,     0.98096,     0.98065,     0.98034,     0.98007,     0.98052,     0.98082,     0.98078,     0.98074,      0.9807,     0.98066,     0.98062,\n",
       "            0.98058,     0.98055,     0.98051,     0.98047,     0.98043,     0.98039,     0.98035,     0.98031,     0.98027,     0.98023,      0.9802,     0.98016,     0.98012,     0.98008,     0.98004,     0.97994,     0.97973,     0.97953,     0.97933,     0.97911,     0.97887,     0.97863,     0.97838,\n",
       "             0.9777,      0.9774,      0.9772,     0.97699,     0.97679,     0.97647,     0.97613,     0.97572,     0.97519,     0.97479,     0.97442,     0.97397,     0.97347,      0.9725,     0.97211,     0.97154,     0.97086,     0.97067,     0.97048,     0.97029,     0.96996,     0.96857,     0.96811,\n",
       "            0.96773,      0.9679,     0.96825,     0.96806,     0.96682,       0.967,     0.96726,     0.96752,     0.96674,     0.96616,     0.96626,     0.96442,     0.96296,     0.96149,     0.95961,     0.95847,     0.95685,     0.95559,     0.95415,     0.95182,     0.94653,     0.94025,     0.93666,\n",
       "             0.9304,     0.92541,     0.92184,     0.91719,      0.9081,     0.90349,      0.8905,     0.88432,     0.87788,     0.86729,     0.85959,     0.85077,     0.84003,      0.8312,     0.81816,     0.80583,     0.79058,     0.77626,     0.76524,     0.74474,     0.73186,     0.71531,     0.70207,\n",
       "             0.6888,     0.68122,     0.66136,     0.64164,     0.61182,     0.58683,     0.57558,     0.56631,     0.55348,     0.53344,     0.49915,     0.48061,     0.43614,     0.37215,     0.33367,     0.28142,     0.20888,     0.12945,    0.064231,    0.032626,    0.016428,   0.0060569,   0.0053016,\n",
       "          0.0045457,   0.0037893,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.88467,     0.88467,     0.91404,     0.92525,     0.93649,     0.93888,     0.94442,      0.9451,     0.94583,     0.94932,     0.95125,     0.95221,      0.9532,     0.95515,     0.95619,     0.95762,     0.95869,     0.96024,      0.9602,     0.96031,     0.96102,     0.96168,     0.96211,\n",
       "            0.96255,     0.96298,     0.96305,     0.96302,     0.96467,     0.96487,     0.96506,     0.96526,     0.96546,     0.96565,     0.96585,     0.96604,     0.96622,     0.96639,     0.96656,     0.96674,     0.96691,     0.96709,     0.96726,     0.96743,     0.96757,     0.96768,      0.9678,\n",
       "            0.96791,     0.96803,     0.96814,     0.96826,     0.96838,     0.96849,     0.96861,     0.96872,     0.96884,     0.96895,     0.96906,     0.96916,     0.96925,     0.96935,     0.96945,     0.96955,     0.96965,     0.96975,     0.96985,     0.96995,     0.97005,     0.97015,     0.97025,\n",
       "            0.97035,     0.97045,      0.9705,      0.9705,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97049,     0.97048,     0.97048,     0.97048,     0.97048,     0.97048,     0.97048,     0.97048,     0.97048,     0.97048,\n",
       "            0.97047,     0.97047,     0.97047,     0.97047,     0.97047,     0.97047,     0.97047,     0.97047,     0.97047,     0.97047,     0.97046,     0.97046,     0.97046,     0.97046,     0.97046,     0.97046,     0.97046,     0.97046,     0.97046,     0.97046,     0.97045,     0.97045,     0.97045,\n",
       "            0.97045,      0.9705,     0.97055,     0.97061,     0.97066,     0.97072,     0.97077,     0.97083,     0.97088,     0.97094,       0.971,     0.97105,     0.97111,     0.97116,     0.97122,     0.97127,     0.97133,     0.97138,     0.97144,     0.97149,     0.97155,     0.97161,     0.97166,\n",
       "            0.97172,     0.97177,     0.97183,     0.97188,     0.97194,     0.97225,     0.97273,     0.97322,     0.97351,     0.97358,     0.97365,     0.97372,     0.97378,     0.97385,     0.97392,     0.97399,     0.97406,     0.97412,     0.97419,     0.97426,     0.97433,      0.9744,     0.97447,\n",
       "            0.97453,      0.9746,     0.97467,     0.97474,     0.97481,     0.97487,     0.97494,       0.975,       0.975,       0.975,       0.975,       0.975,     0.97499,     0.97499,     0.97499,     0.97499,     0.97499,     0.97499,     0.97499,     0.97499,     0.97499,     0.97499,     0.97499,\n",
       "            0.97498,     0.97498,     0.97498,     0.97498,     0.97498,     0.97498,     0.97498,     0.97498,     0.97498,     0.97498,     0.97497,     0.97497,     0.97497,     0.97497,     0.97497,     0.97497,     0.97497,     0.97497,     0.97497,     0.97497,     0.97496,     0.97496,     0.97496,\n",
       "            0.97496,     0.97496,     0.97499,     0.97501,     0.97504,     0.97506,     0.97509,     0.97511,     0.97514,     0.97516,     0.97519,     0.97521,     0.97524,     0.97526,     0.97528,     0.97531,     0.97533,     0.97536,     0.97538,     0.97541,     0.97543,     0.97546,     0.97548,\n",
       "            0.97551,     0.97553,     0.97556,     0.97558,      0.9756,     0.97563,     0.97565,     0.97568,      0.9757,     0.97573,     0.97575,     0.97578,      0.9758,     0.97583,     0.97585,     0.97588,      0.9759,     0.97592,     0.97595,     0.97597,       0.976,     0.97602,     0.97605,\n",
       "            0.97607,      0.9761,     0.97612,     0.97615,     0.97617,      0.9762,     0.97622,     0.97625,     0.97627,     0.97629,     0.97632,     0.97634,     0.97637,     0.97639,     0.97642,     0.97644,     0.97647,      0.9765,     0.97657,     0.97664,     0.97671,     0.97678,     0.97685,\n",
       "            0.97692,     0.97699,     0.97706,     0.97714,     0.97721,     0.97728,     0.97735,     0.97742,     0.97749,     0.97756,     0.97763,      0.9777,     0.97778,     0.97785,     0.97792,     0.97799,     0.97802,     0.97802,     0.97802,     0.97802,     0.97802,     0.97801,     0.97801,\n",
       "            0.97801,     0.97801,     0.97801,     0.97801,     0.97801,     0.97801,       0.978,       0.978,       0.978,       0.978,       0.978,       0.978,       0.978,       0.978,     0.97799,     0.97799,     0.97799,     0.97799,     0.97799,     0.97799,     0.97817,     0.97875,     0.97932,\n",
       "            0.97986,     0.98038,      0.9809,     0.98109,     0.98111,     0.98113,     0.98115,     0.98117,     0.98119,     0.98121,     0.98123,     0.98125,     0.98127,     0.98129,     0.98131,     0.98133,     0.98135,     0.98137,     0.98139,     0.98141,     0.98143,     0.98145,     0.98147,\n",
       "            0.98149,     0.98151,     0.98153,     0.98155,     0.98157,     0.98159,     0.98161,     0.98163,     0.98165,     0.98167,     0.98169,     0.98171,     0.98173,     0.98175,     0.98177,     0.98179,     0.98181,     0.98183,     0.98185,     0.98187,     0.98189,     0.98191,     0.98193,\n",
       "            0.98195,     0.98197,     0.98199,     0.98201,     0.98203,     0.98205,     0.98207,     0.98209,     0.98211,     0.98213,     0.98215,     0.98217,     0.98219,     0.98221,     0.98223,     0.98225,     0.98227,     0.98229,     0.98231,     0.98233,     0.98235,     0.98237,     0.98239,\n",
       "            0.98241,     0.98243,     0.98245,     0.98247,      0.9825,     0.98252,     0.98254,     0.98256,     0.98258,      0.9826,     0.98262,     0.98263,     0.98265,     0.98266,     0.98268,     0.98269,      0.9827,     0.98272,     0.98273,     0.98275,     0.98276,     0.98278,     0.98279,\n",
       "            0.98281,     0.98282,     0.98283,     0.98285,     0.98286,     0.98288,     0.98289,     0.98291,     0.98292,     0.98294,     0.98295,     0.98296,     0.98298,     0.98299,     0.98301,     0.98302,     0.98304,     0.98305,     0.98307,     0.98308,     0.98309,     0.98311,     0.98312,\n",
       "            0.98314,     0.98315,     0.98317,     0.98318,      0.9832,     0.98321,     0.98322,     0.98324,     0.98325,     0.98327,     0.98328,      0.9833,     0.98331,     0.98333,     0.98334,     0.98335,     0.98337,     0.98338,      0.9834,     0.98341,     0.98343,     0.98344,     0.98346,\n",
       "            0.98347,     0.98348,      0.9835,     0.98351,     0.98353,     0.98354,     0.98356,     0.98357,     0.98359,      0.9836,     0.98361,     0.98363,     0.98364,     0.98366,     0.98367,     0.98369,      0.9837,     0.98372,     0.98373,     0.98374,     0.98376,     0.98377,     0.98379,\n",
       "             0.9838,     0.98382,     0.98383,     0.98385,     0.98386,     0.98387,     0.98389,      0.9839,     0.98392,     0.98393,     0.98395,     0.98396,     0.98398,     0.98399,       0.984,     0.98402,     0.98403,     0.98405,     0.98406,     0.98408,     0.98409,     0.98411,     0.98412,\n",
       "            0.98413,     0.98415,     0.98416,     0.98418,     0.98417,     0.98417,     0.98417,     0.98417,     0.98417,     0.98416,     0.98416,     0.98416,     0.98416,     0.98415,     0.98415,     0.98418,     0.98421,     0.98423,     0.98426,     0.98428,     0.98431,     0.98433,     0.98436,\n",
       "            0.98438,     0.98441,     0.98444,     0.98446,     0.98449,     0.98451,     0.98454,     0.98456,     0.98459,     0.98461,     0.98464,     0.98467,     0.98469,     0.98472,     0.98474,     0.98477,     0.98479,     0.98482,     0.98485,     0.98487,      0.9849,     0.98492,     0.98495,\n",
       "            0.98497,       0.985,     0.98502,     0.98505,     0.98508,      0.9851,     0.98513,     0.98515,     0.98518,      0.9852,     0.98523,     0.98525,     0.98528,     0.98531,     0.98533,     0.98536,     0.98538,     0.98541,     0.98543,     0.98546,     0.98548,     0.98551,     0.98554,\n",
       "            0.98556,     0.98559,     0.98561,     0.98564,     0.98566,     0.98569,     0.98571,     0.98575,     0.98579,     0.98583,     0.98587,     0.98591,     0.98595,     0.98599,     0.98603,     0.98607,     0.98611,     0.98615,     0.98619,     0.98622,     0.98626,      0.9863,     0.98634,\n",
       "            0.98638,     0.98642,     0.98646,      0.9865,     0.98654,     0.98658,     0.98662,     0.98666,      0.9867,     0.98674,     0.98677,     0.98681,     0.98685,     0.98689,     0.98693,     0.98697,     0.98701,     0.98705,     0.98709,     0.98713,     0.98717,     0.98721,     0.98725,\n",
       "            0.98728,      0.9873,     0.98732,     0.98734,     0.98736,     0.98738,      0.9874,     0.98742,     0.98744,     0.98746,     0.98748,      0.9875,     0.98752,     0.98754,     0.98756,     0.98758,      0.9876,     0.98763,     0.98765,     0.98767,     0.98769,     0.98771,     0.98773,\n",
       "            0.98775,     0.98777,     0.98779,     0.98781,     0.98783,     0.98785,     0.98787,     0.98789,     0.98791,     0.98793,     0.98795,     0.98797,     0.98799,     0.98801,     0.98803,     0.98805,     0.98807,     0.98809,     0.98811,     0.98813,     0.98815,     0.98817,     0.98819,\n",
       "            0.98821,     0.98823,     0.98825,     0.98827,     0.98829,     0.98831,     0.98833,     0.98835,     0.98837,     0.98839,     0.98841,     0.98843,     0.98845,     0.98847,     0.98849,     0.98851,     0.98853,     0.98855,     0.98857,     0.98859,     0.98861,     0.98863,     0.98865,\n",
       "            0.98867,     0.98869,     0.98871,     0.98873,     0.98875,     0.98877,     0.98879,     0.98881,     0.98883,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,\n",
       "            0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,     0.98885,\n",
       "            0.98885,     0.98885,     0.98885,     0.98885,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,\n",
       "            0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,     0.98884,\n",
       "            0.98884,     0.98884,     0.98884,      0.9889,     0.98904,     0.98917,     0.98931,     0.98944,     0.98957,     0.98971,     0.98984,     0.98997,     0.99011,     0.99024,     0.99038,     0.99041,     0.99041,     0.99041,     0.99041,      0.9904,      0.9904,      0.9904,     0.99039,\n",
       "            0.99039,     0.99039,     0.99038,     0.99038,     0.99037,     0.99037,     0.99037,     0.99037,     0.99037,     0.99037,     0.99037,     0.99037,     0.99037,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,\n",
       "            0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99035,     0.99035,     0.99035,     0.99035,     0.99034,     0.99033,     0.99033,     0.99036,     0.99129,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,\n",
       "            0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99192,     0.99191,     0.99191,     0.99191,     0.99191,     0.99191,     0.99191,     0.99191,     0.99191,     0.99191,     0.99191,      0.9919,      0.9919,      0.9919,     0.99189,     0.99189,     0.99189,     0.99188,\n",
       "            0.99187,     0.99187,     0.99186,     0.99186,     0.99186,     0.99185,     0.99185,     0.99184,     0.99183,     0.99183,     0.99182,     0.99181,      0.9918,     0.99179,     0.99178,     0.99177,     0.99176,     0.99176,     0.99175,     0.99175,     0.99175,     0.99172,     0.99172,\n",
       "            0.99171,     0.99226,     0.99299,     0.99335,     0.99333,     0.99386,     0.99441,     0.99496,     0.99498,     0.99498,     0.99664,     0.99663,     0.99662,      0.9983,     0.99829,     0.99829,     0.99828,     0.99828,     0.99827,     0.99827,     0.99825,     0.99823,     0.99821,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.99368,     0.99368,      0.9921,      0.9921,      0.9921,      0.9921,      0.9921,      0.9921,      0.9921,      0.9921,      0.9921,      0.9921,      0.9921,      0.9921,      0.9921,      0.9921,      0.9921,     0.99209,     0.99093,     0.98894,     0.98894,     0.98894,     0.98894,\n",
       "            0.98894,     0.98894,      0.9883,     0.98749,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,\n",
       "            0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,     0.98736,\n",
       "            0.98736,     0.98736,     0.98734,     0.98731,     0.98727,     0.98724,      0.9872,     0.98717,     0.98713,      0.9871,     0.98706,     0.98703,     0.98699,     0.98696,     0.98692,     0.98688,     0.98685,     0.98681,     0.98678,     0.98674,     0.98671,     0.98667,     0.98664,\n",
       "             0.9866,     0.98657,     0.98653,      0.9865,     0.98646,     0.98643,     0.98639,     0.98635,     0.98632,     0.98628,     0.98625,     0.98621,     0.98618,     0.98614,     0.98611,     0.98607,     0.98604,       0.986,     0.98597,     0.98593,      0.9859,     0.98586,     0.98582,\n",
       "            0.98579,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,\n",
       "            0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,\n",
       "            0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98578,     0.98574,      0.9857,     0.98566,     0.98562,     0.98558,     0.98554,      0.9855,     0.98546,     0.98542,     0.98538,     0.98534,      0.9853,     0.98526,     0.98522,     0.98518,\n",
       "            0.98514,      0.9851,     0.98507,     0.98503,     0.98499,     0.98495,     0.98491,     0.98487,     0.98483,     0.98479,     0.98475,     0.98471,     0.98467,     0.98463,     0.98459,     0.98455,     0.98451,     0.98447,     0.98443,     0.98439,     0.98435,     0.98432,     0.98428,\n",
       "            0.98424,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,\n",
       "             0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,\n",
       "             0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,\n",
       "             0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,      0.9842,     0.98417,     0.98411,     0.98406,       0.984,     0.98394,     0.98388,     0.98382,\n",
       "            0.98377,     0.98371,     0.98365,     0.98359,     0.98353,     0.98347,     0.98342,     0.98336,      0.9833,     0.98324,     0.98318,     0.98313,     0.98307,     0.98301,     0.98295,     0.98289,     0.98284,     0.98278,     0.98272,     0.98266,     0.98262,     0.98262,     0.98262,\n",
       "            0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,\n",
       "            0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,\n",
       "            0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,\n",
       "            0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,\n",
       "            0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,\n",
       "            0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,\n",
       "            0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,\n",
       "            0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,     0.98262,\n",
       "            0.98262,     0.98262,     0.98262,     0.98262,     0.98247,     0.98233,     0.98218,     0.98204,      0.9819,     0.98175,     0.98161,     0.98146,     0.98132,     0.98117,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,\n",
       "            0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,\n",
       "            0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,\n",
       "            0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,\n",
       "            0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,\n",
       "            0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,\n",
       "            0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,\n",
       "            0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,\n",
       "            0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98104,     0.98103,     0.98101,     0.98099,     0.98097,     0.98095,     0.98093,     0.98092,      0.9809,     0.98088,     0.98086,     0.98084,     0.98082,      0.9808,\n",
       "            0.98079,     0.98077,     0.98075,     0.98073,     0.98071,     0.98069,     0.98067,     0.98066,     0.98064,     0.98062,      0.9806,     0.98058,     0.98056,     0.98055,     0.98053,     0.98051,     0.98049,     0.98047,     0.98045,     0.98043,     0.98042,      0.9804,     0.98038,\n",
       "            0.98036,     0.98034,     0.98032,      0.9803,     0.98029,     0.98027,     0.98025,     0.98023,     0.98021,     0.98019,     0.98018,     0.98016,     0.98014,     0.98012,      0.9801,     0.98008,     0.98006,     0.98005,     0.98003,     0.98001,     0.97999,     0.97997,     0.97995,\n",
       "            0.97993,     0.97992,      0.9799,     0.97988,     0.97986,     0.97984,     0.97982,     0.97981,     0.97979,     0.97977,     0.97975,     0.97973,     0.97971,     0.97969,     0.97968,     0.97966,     0.97964,     0.97962,      0.9796,     0.97958,     0.97956,     0.97955,     0.97953,\n",
       "            0.97951,     0.97949,     0.97947,     0.97946,     0.97946,     0.97946,     0.97946,     0.97946,     0.97946,     0.97946,     0.97946,     0.97946,     0.97946,     0.97946,     0.97946,     0.97926,     0.97898,      0.9787,     0.97842,     0.97813,     0.97785,     0.97753,     0.97721,\n",
       "            0.97689,     0.97657,     0.97621,     0.97558,     0.97495,     0.97469,     0.97463,     0.97458,     0.97452,     0.97447,     0.97441,     0.97435,      0.9743,     0.97424,     0.97419,     0.97413,     0.97408,     0.97402,     0.97396,     0.97391,     0.97385,      0.9738,     0.97374,\n",
       "            0.97369,     0.97363,     0.97358,     0.97352,     0.97346,     0.97341,     0.97335,      0.9733,     0.97324,     0.97319,     0.97299,     0.97238,     0.97176,     0.97116,     0.97056,     0.96998,     0.96998,     0.96996,     0.96989,     0.96981,     0.96973,     0.96966,     0.96958,\n",
       "            0.96951,     0.96943,     0.96936,     0.96928,     0.96921,     0.96913,     0.96905,     0.96898,      0.9689,     0.96883,     0.96875,     0.96868,      0.9686,     0.96853,     0.96845,     0.96825,     0.96786,     0.96747,     0.96708,     0.96666,     0.96619,     0.96572,     0.96525,\n",
       "            0.96393,     0.96335,     0.96296,     0.96256,     0.96217,     0.96156,     0.96091,     0.96011,      0.9591,     0.95834,     0.95763,     0.95676,      0.9558,     0.95395,     0.95321,     0.95212,     0.95083,     0.95047,      0.9501,     0.94974,     0.94911,     0.94647,      0.9456,\n",
       "            0.94488,     0.94471,     0.94471,     0.94402,     0.94169,     0.94155,     0.94155,     0.94155,     0.94006,     0.93896,     0.93768,     0.93423,      0.9315,      0.9273,     0.92382,      0.9217,     0.91873,      0.9164,     0.91377,      0.9095,      0.8999,     0.88864,     0.88227,\n",
       "            0.86986,     0.86118,     0.85501,     0.84704,     0.83166,     0.82397,     0.80262,     0.79263,     0.78233,     0.76568,     0.75376,      0.7403,     0.72418,     0.71115,     0.69228,      0.6748,     0.65369,     0.63433,     0.61975,     0.59329,     0.57712,     0.55679,     0.54092,\n",
       "            0.52532,     0.51655,     0.49405,     0.47236,     0.44073,     0.41526,     0.40408,       0.395,     0.38263,     0.36374,     0.33258,     0.31632,     0.27889,     0.22861,     0.20025,     0.16375,     0.11662,    0.069204,    0.033181,    0.016584,   0.0082821,   0.0030376,   0.0026578,\n",
       "          0.0022781,   0.0018983,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: 0.928854611784508\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.92161])\n",
       "names: {0: 'plate'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.9888531112059233, 'metrics/recall(B)': 0.9810078212592139, 'metrics/mAP50(B)': 0.9940650538182327, 'metrics/mAP50-95(B)': 0.9216090071140941, 'fitness': 0.928854611784508}\n",
       "save_dir: PosixPath('runs/detect/train72')\n",
       "speed: {'preprocess': 0.17211614720058632, 'inference': 14.719328410188917, 'loss': 0.0020167237556196556, 'postprocess': 2.9550351847105585}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9925275e-5f81-42e0-bb8a-6023917e6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torchvision.ops import batched_nms, nms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f33847-99bb-4623-8631-16f8ba66eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_postprocess(data: Tuple[Tensor, Tensor, Tensor, Tensor]):\n",
    "    assert len(data) == 4\n",
    "    iou_thres: float = 0.65\n",
    "    num_dets, bboxes, scores, labels = data[0][0], data[1][0], data[2][\n",
    "        0], data[3][0]\n",
    "    nums = num_dets.item()\n",
    "    if nums == 0:\n",
    "        return bboxes.new_zeros((0, 4)), scores.new_zeros(\n",
    "            (0, )), labels.new_zeros((0, ))\n",
    "    # check score negative\n",
    "    scores[scores < 0] = 1 + scores[scores < 0]\n",
    "    # add nms\n",
    "    idx = nms(bboxes, scores, iou_thres)\n",
    "    bboxes, scores, labels = bboxes[idx], scores[idx], labels[idx]\n",
    "    bboxes = bboxes[:nums]\n",
    "    scores = scores[:nums]\n",
    "    labels = labels[:nums]\n",
    "\n",
    "    return bboxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ecfcf6-096f-4684-8ed7-9598dad6b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plate_recognizer import PlateRecognizer\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82cf4e79-1d53-4012-b643-19d0f28bd676",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = utils.read_config()\n",
    "plate_recognizer = PlateRecognizer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab3081-9f51-46a5-b417-7f2164838eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import os, cv2\n",
    "\n",
    "test_dir = os.listdir('/home/duc-softzone/.cache/kagglehub/datasets/duydieunguyen/licenseplates/versions/1/images/val')\n",
    "for file in test_dir:\n",
    "    path = os.path.join('/home/duc-softzone/.cache/kagglehub/datasets/duydieunguyen/licenseplates/versions/1/images/val', file)\n",
    "    image = cv2.imread(path)\n",
    "    plate_recognizer.recog(image)\n",
    "    # image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # results = model.predict(image)\n",
    "    # for result in results:\n",
    "    #     draw = image_rgb.copy()\n",
    "    #     for box in result.boxes:\n",
    "            \n",
    "    #         draw = cv2.rectangle(draw, (int(box.xyxy[0][0]), int(box.xyxy[0][1])),\n",
    "    #                       (int(box.xyxy[0][2]), int(box.xyxy[0][3])), (255, 0, 0), 2)\n",
    "    #         draw = cv2.putText(draw, f\"{result.names[int(box.cls[0])]} | {box.conf[0]:.2f}\",\n",
    "    #                     (int(box.xyxy[0][0]), int(box.xyxy[0][1]) - 10),\n",
    "    #                     cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0), 1)\n",
    "    #     plt.imshow(draw)\n",
    "    #     plt.axis('off')\n",
    "    #     plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b431ce8-614f-44df-971c-7be265d8b3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
